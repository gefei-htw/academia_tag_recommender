{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "This notebook evaluates methods for embedded document representation using the [academia.stackexchange.com](https://academia.stackexchange.com/) data dump.\n",
    "\n",
    "## Table of Contents\n",
    "* [Data import](#data_import)\n",
    "* [Embedding](#embedding)\n",
    "* [Experiments](#experiments)\n",
    "* [Evaluation](#evaluation)\n",
    "* [Dimension Reduction](#dim_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from academia_tag_recommender.definitions import MODELS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_import'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academia_tag_recommender.data import documents\n",
    "texts = [document.text for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set with shape  (24812, 1)\n",
      "Test set with shape (8270, 1)\n"
     ]
    }
   ],
   "source": [
    "from academia_tag_recommender.test_train_data import get_y, get_test_train_data, get_all_labels\n",
    "\n",
    "X = np.vstack([document.text for document in documents])\n",
    "y = get_y()\n",
    "labels = get_all_labels(reduced=False)\n",
    "X_train, X_test, y_train, y_test = get_test_train_data(X, y, scale=False)\n",
    "print('Train set with shape ', X_train.shape)\n",
    "print('Test set with shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='embedding'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "> Word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. [Wikipedia contributors (2021)][1]\n",
    "\n",
    "The following word embedding models will be for this approach:\n",
    "- Word2Vec\n",
    "- Doc2Vec\n",
    "- FastText\n",
    "\n",
    "\n",
    "**Word2Vec**\n",
    "> The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications. [Google (2013)][2]\n",
    "\n",
    "\n",
    "**Doc2Vec**\n",
    "> Le and Mikolov in 2014 introduced the Doc2Vec algorithm, which usually outperforms such simple-averaging of Word2Vec vectors. The basic idea is: act as if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. [Radim Řehůřek (2020)][3]\n",
    "\n",
    "\n",
    "**FastText**\n",
    "> The main principle behind [F]astText is that the morphological structure of a word carries important information about the meaning of the word. Such structure is not taken into account by traditional word embeddings like Word2Vec, which train a unique word embedding for every individual word. [F]astText attempts to solve this by treating each word as the aggregation of its subwords. For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams. [Radim Řehůřek (2020)][4]\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Word_embedding\n",
    "[2]: https://code.google.com/archive/p/word2vec/\n",
    "[3]: https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py\n",
    "[4]: https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experiments'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Before preprocessing a document is still a whole sentence, including punctuation and html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Where can I find the Impact Factor for a given journal? <p>As from title. Not all journals provide the impact factor on their homepage. For those who don't where can I find their impact factor ?</p>\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**\n",
    "\n",
    "First the string sentences are tokenized into arrays of strings representing the words. Punctuation and html tags are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'find', 'impact', 'factor', 'given', 'journal', 'title', 'journals', 'provide', 'impact', 'factor', 'homepage', 'don', 'can', 'find', 'impact', 'factor']\n"
     ]
    }
   ],
   "source": [
    "from academia_tag_recommender.embedded_data import Word2Tok\n",
    "sentences = Word2Tok(X_train)\n",
    "\n",
    "print(list(sentences)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Word2Vec` model is trained using the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on the documents results in 12894 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=sentences)\n",
    "wv = model.wv\n",
    "del model\n",
    "wv.init_sims(replace=True)\n",
    "print('Training the model on the documents results in {} words in the vocabulary.'.format(len(wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Word2Vec` can now be used to generate a vectors for words. Per default implementation the vector has 100 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21820764,  0.00911908,  0.04361239,  0.06080858, -0.07216044,\n",
       "       -0.11958563, -0.13394633, -0.04513133,  0.14242645,  0.09354336,\n",
       "       -0.10020303, -0.04972231,  0.17514479, -0.0194051 ,  0.05589714,\n",
       "        0.06976553,  0.05294935, -0.06892349,  0.10797131,  0.10748126,\n",
       "        0.00488774, -0.0973985 , -0.09980931, -0.05023666, -0.11743355,\n",
       "       -0.0603033 ,  0.07642061, -0.05857554, -0.04397227,  0.12190309,\n",
       "       -0.02885075, -0.07793532,  0.1043091 ,  0.10896602,  0.19491932,\n",
       "       -0.04871687, -0.12617314,  0.16792402,  0.01798099, -0.00246853,\n",
       "        0.05683499, -0.05630356, -0.07650578, -0.01946483,  0.05033841,\n",
       "       -0.23604739, -0.04106637, -0.08350789,  0.06483133,  0.10227551,\n",
       "        0.21002582, -0.02293057,  0.12989593,  0.14919503,  0.3716091 ,\n",
       "       -0.0404757 , -0.16821301,  0.06272127,  0.10951851, -0.01312884,\n",
       "        0.01802835,  0.04073705,  0.0381859 ,  0.08215148,  0.05578558,\n",
       "       -0.11915142, -0.22632273, -0.05271645, -0.03897511,  0.02879327,\n",
       "        0.05002899,  0.15912142, -0.03840779,  0.03718755,  0.04035228,\n",
       "       -0.0214678 , -0.10742392,  0.06260817,  0.01193082,  0.0159421 ,\n",
       "       -0.03151789,  0.04659424,  0.09685753,  0.09943683, -0.03548301,\n",
       "        0.03927346,  0.07795732,  0.07894841, -0.06287417, -0.10871393,\n",
       "        0.00816297,  0.11370932, -0.0042003 ,  0.07095031,  0.12410193,\n",
       "       -0.05889275, -0.12295861,  0.10709674,  0.06565835,  0.01349274],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['academic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these vectors words can be compared to each other. The 10 most similar words to `academic` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scientific', 0.5422207117080688),\n",
       " ('professional', 0.5357193350791931),\n",
       " ('existent', 0.4653562903404236),\n",
       " ('academia', 0.447778582572937),\n",
       " ('prospects', 0.434415727853775),\n",
       " ('academics', 0.43376612663269043),\n",
       " ('permanent', 0.4279685616493225),\n",
       " ('traditional', 0.4223255515098572),\n",
       " ('future', 0.4221450090408325),\n",
       " ('educational', 0.4168570041656494)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Word2Vec with multiword n-grams**\n",
    "\n",
    "Instead of only using unigrams it is possible to include bigrams into the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'find', 'impact', 'factor', 'given', 'journal', 'title', 'journals', 'provide', 'impact', 'factor', 'homepage', 'don', 'can', 'find', 'impact', 'factor']\n"
     ]
    }
   ],
   "source": [
    "from academia_tag_recommender.embedded_data import Word2Tok\n",
    "sentences = Word2Tok(X_train)\n",
    "\n",
    "print(list(sentences)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "bigram_transformer = Phrases(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on the documents results in 20947 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=bigram_transformer[sentences])\n",
    "wv = model.wv\n",
    "del model\n",
    "wv.init_sims(replace=True)\n",
    "print('Training the model on the documents results in {} words in the vocabulary.'.format(len(wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including bigrams the vocabulary increases nearly by factor 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14137234,  0.00499952,  0.05469143, -0.03796696, -0.07623484,\n",
       "        0.1899787 , -0.15662064,  0.00240947,  0.05156227,  0.14496706,\n",
       "        0.05527455,  0.01662143,  0.03960598, -0.08055399,  0.01805119,\n",
       "        0.08793066, -0.04509585, -0.06955174,  0.12589985, -0.05598166,\n",
       "       -0.03167972, -0.05352683,  0.03945947, -0.06907973, -0.084567  ,\n",
       "        0.01734045,  0.14473717, -0.17430982, -0.23063143,  0.09975462,\n",
       "       -0.06112049,  0.00801671,  0.03474763,  0.1771201 ,  0.05513798,\n",
       "       -0.02076746, -0.05675825,  0.1186769 , -0.02672635, -0.00430646,\n",
       "        0.03073694,  0.06659339, -0.08348006, -0.1133301 ,  0.02486548,\n",
       "       -0.16794343,  0.03303879, -0.2016477 ,  0.10804214,  0.03066116,\n",
       "        0.11334135, -0.01516772, -0.00446554,  0.04920239,  0.18825074,\n",
       "       -0.04130374, -0.12770376,  0.03981686,  0.11411712,  0.05911706,\n",
       "        0.03632458, -0.15409419,  0.22848092,  0.03293614,  0.05623715,\n",
       "       -0.1329605 ,  0.01865764,  0.0248922 ,  0.02707001, -0.01129523,\n",
       "        0.05232851,  0.1487135 ,  0.08450273,  0.06018127, -0.00269387,\n",
       "        0.05116879, -0.1313444 ,  0.22826959, -0.11254419, -0.01681467,\n",
       "        0.13438436,  0.06355221,  0.10657007,  0.05446984, -0.03225668,\n",
       "       -0.20159301,  0.06294454, -0.11242738, -0.14498557, -0.14889467,\n",
       "       -0.19526877,  0.00961122,  0.09307565,  0.0127612 , -0.10151119,\n",
       "       -0.06114886, -0.13121423, -0.04492854,  0.11310613,  0.04566408],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['academic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('non_academic', 0.7226566076278687),\n",
       " ('early_career', 0.6944350004196167),\n",
       " ('professional', 0.6883115768432617),\n",
       " ('permanent', 0.6243336200714111),\n",
       " ('listing', 0.6010950207710266),\n",
       " ('soft_money', 0.5930420160293579),\n",
       " ('experiences', 0.5891789197921753),\n",
       " ('indigenous', 0.586772084236145),\n",
       " ('job_market', 0.5856984853744507),\n",
       " ('industrial', 0.582744836807251)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the simililarity of words, there are now many bigrams that are similar to `academic`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Doc2Vec` trains the model supervised, using the labels. Therefore all documents first need to be tokenized and connected to their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['can', 'find', 'impact', 'factor', 'given', 'journal', 'title', 'journals', 'provide', 'impact', 'factor', 'homepage', 'don', 'can', 'find', 'impact', 'factor'], tags=[0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from academia_tag_recommender.embedded_data import Doc2Tagged\n",
    "\n",
    "tokens = Doc2Tagged(X_train, tag=True)\n",
    "\n",
    "token_list = list(tokens)\n",
    "token_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on the documents results in 12894 words out of 24812 documents in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec()\n",
    "model.build_vocab(token_list)\n",
    "model.train(token_list, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print('Training the model on the documents results in {} words out of {} documents in the vocabulary.'.format(len(model.wv.vocab), len(model.docvecs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01438004, -0.01514934, -0.01238022,  0.04506659, -0.00378427,\n",
       "        0.00355258, -0.05222021,  0.02039241, -0.0093652 ,  0.01598098,\n",
       "       -0.01317822, -0.00817952, -0.0455081 ,  0.01327964,  0.02089488,\n",
       "        0.00648432, -0.0039745 , -0.02725293,  0.00987052, -0.00451604,\n",
       "       -0.01583723,  0.01115874, -0.04272858, -0.03579678,  0.04536224,\n",
       "       -0.03366986, -0.00314346, -0.0288485 , -0.04700197, -0.00745611,\n",
       "        0.01262811, -0.02878716, -0.01851433, -0.00694802, -0.02387403,\n",
       "        0.01044197, -0.03539443,  0.03339054,  0.00021077,  0.02185567,\n",
       "        0.02020962, -0.00454351,  0.04668414, -0.05026869, -0.0239145 ,\n",
       "       -0.02871663,  0.02794001,  0.00298674,  0.00213374,  0.06018677,\n",
       "       -0.01920959,  0.01200431,  0.01375066,  0.01035046,  0.01845757,\n",
       "        0.0004169 , -0.05598499, -0.00935971,  0.05087708,  0.08599964,\n",
       "       -0.01151139, -0.05269783,  0.01051827, -0.02546464,  0.02787663,\n",
       "        0.04050624,  0.02498539,  0.03593248,  0.02138111, -0.01701461,\n",
       "       -0.01589511,  0.00996159,  0.02011003, -0.00370619,  0.00797547,\n",
       "       -0.00176892, -0.06318692, -0.04224196, -0.01441196, -0.00373312,\n",
       "       -0.03411557, -0.00248668, -0.02193388, -0.00138231, -0.00030049,\n",
       "       -0.04771678,  0.02270181, -0.05374838, -0.05880279,  0.00483997,\n",
       "       -0.00376324,  0.01338142,  0.00430177, -0.01039801, -0.02711026,\n",
       "        0.03036744, -0.02009736,  0.03287579,  0.04624048,  0.00282354],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.infer_vector(['academic'])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `Doc2Vec` model one can examine documents similar to the keyword `academic` instead of similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17937, 0.8849472403526306) ['academic', 'prizes', 'awards', 'vs', 'academic', 'achievements', 'phd', 'scholarship', 'application', 'forms', 'mean', 'details', 'academic', 'prizes', 'awards', 'details', 'academic', 'achievements', 'academic', 'prizes', 'academic', 'achievements', 'things']\n",
      "(2684, 0.8624860644340515) ['applying', 'masters', 'degree', 'academic', 'integrity', 'violation', 'record', 'junior', 'college', 'student', 'goes', 'top', 'cs', 'school', 'will', 'graduating', 'year', 'early', 'starting', 'think', 'whether', 'want', 'get', 'master', 'degree', 'however', 'egregious', 'academic', 'integrity', 'violation', 'first', 'university', 'expelled', 'top', 'cs', 'school', 'first', 'semester', 'since', 'learned', 'incident', 'transferred', 'current', 'college', 'studying', 'years', 'decide', 'get', 'masters', 'degree', 'either', 'plan', 'pursuing', 'current', 'school', 'believe', 'will', 'accepted', 'due', 'academic', 'performance', 'relationships', 'professors', 'fact', 'department', 'head', 'knows', 'past', 'another', 'top', 'cs', 'school', 'school', 'caliber', 'willing', 'look', 'past', 'academic', 'integrity', 'violation', 'even', 'bother', 'applying', 'master', 'degree']\n"
     ]
    }
   ],
   "source": [
    "similar_docs = model.docvecs.most_similar([vector])\n",
    "print(similar_docs[0], token_list[similar_docs[0][0]].words)\n",
    "print(similar_docs[1], token_list[similar_docs[1][0]].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'find', 'impact', 'factor', 'given', 'journal', 'title', 'journals', 'provide', 'impact', 'factor', 'homepage', 'don', 'can', 'find', 'impact', 'factor']\n"
     ]
    }
   ],
   "source": [
    "from academia_tag_recommender.embedded_data import Word2Tok\n",
    "sentences = Word2Tok(X_train)\n",
    "\n",
    "print(list(sentences)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on the documents results in 22407 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(window=3, min_count=2)\n",
    "model.build_vocab(sentences=sentences)\n",
    "model.train(sentences=sentences, total_examples=model.corpus_count, epochs=20)\n",
    "wv = model.wv\n",
    "del model\n",
    "wv.init_sims(replace=True)\n",
    "print('Training the model on the documents results in {} words in the vocabulary.'.format(len(wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07453362,  0.06943832,  0.07596593,  0.01869199, -0.01562418,\n",
       "        0.00698225,  0.09509411,  0.08842903, -0.14629164,  0.00929409,\n",
       "        0.05402608, -0.04557607, -0.05447316, -0.03358596,  0.00929349,\n",
       "       -0.12571113,  0.04440993,  0.00276269,  0.16809362,  0.06980025,\n",
       "       -0.15784076, -0.04907261,  0.04472274,  0.02718009,  0.01598027,\n",
       "       -0.12894505,  0.04531686, -0.00085054,  0.16506658, -0.22018442,\n",
       "       -0.11810841, -0.05850428,  0.0758732 , -0.02374969, -0.11128239,\n",
       "       -0.05519569,  0.12765019, -0.10749408,  0.11275966,  0.14490195,\n",
       "        0.27204397,  0.08476449, -0.06741492, -0.04653229, -0.0737948 ,\n",
       "       -0.05993982,  0.0353999 , -0.06280465,  0.0188218 , -0.04606069,\n",
       "        0.01873511, -0.03340943,  0.05184827,  0.0497258 , -0.05961163,\n",
       "        0.17493825, -0.15894082,  0.04596837,  0.02607274,  0.12520662,\n",
       "        0.02846302,  0.03607515,  0.07605084, -0.05812393, -0.07155358,\n",
       "       -0.16323806, -0.09102985, -0.03477538, -0.11656777,  0.13176797,\n",
       "        0.04205249,  0.01638487, -0.01201939,  0.14166422, -0.10313684,\n",
       "        0.02960698, -0.08251094, -0.03770507, -0.00638891,  0.05113342,\n",
       "       -0.00655497, -0.19745786,  0.17678279,  0.04174257,  0.07678892,\n",
       "        0.03513595, -0.20521195,  0.06527267, -0.15568577, -0.08423433,\n",
       "        0.07524709,  0.1817848 ,  0.0092589 , -0.11088643,  0.12222327,\n",
       "       -0.18263684,  0.15746811, -0.08770682,  0.04695326,  0.22616921],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['academic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unacademic', 0.9385606050491333),\n",
       " ('nonacademic', 0.9301602840423584),\n",
       " ('academical', 0.8922649621963501),\n",
       " ('academy', 0.8589457273483276),\n",
       " ('academe', 0.8575608134269714),\n",
       " ('academies', 0.8407630920410156),\n",
       " ('academician', 0.8213838338851929),\n",
       " ('acad', 0.8002569675445557),\n",
       " ('academically', 0.7822352647781372),\n",
       " ('academiase', 0.778761625289917)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `FastText` uses character n-grams there are now many words very close to `academia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining FastText with multiword n-grams**\n",
    "\n",
    "`FastText` can be extended with bigrams too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'find', 'impact', 'factor', 'given', 'journal', 'title', 'journals', 'provide', 'impact', 'factor', 'homepage', 'don', 'can', 'find', 'impact', 'factor']\n"
     ]
    }
   ],
   "source": [
    "from academia_tag_recommender.embedded_data import Word2Tok\n",
    "sentences = Word2Tok(X_train)\n",
    "\n",
    "print(list(sentences)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "bigram_transformer = Phrases(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on the documents results in 58252 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(window=3, min_count=2)\n",
    "model.build_vocab(sentences=bigram_transformer[sentences])\n",
    "model.train(sentences=sentences, total_examples=model.corpus_count, epochs=20)\n",
    "wv = model.wv\n",
    "del model\n",
    "wv.init_sims(replace=True)\n",
    "print('Training the model on the documents results in {} words in the vocabulary.'.format(len(wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08802791, -0.06200763,  0.1152458 ,  0.06426968,  0.09748365,\n",
       "        0.05013251, -0.1129626 , -0.09028978,  0.04577056,  0.0607812 ,\n",
       "        0.15716203, -0.03857641, -0.09675916,  0.03481515,  0.22809464,\n",
       "        0.08557537,  0.0505088 ,  0.161692  , -0.12159577, -0.01974001,\n",
       "        0.0351983 ,  0.01888811, -0.02160684,  0.14407273, -0.1854414 ,\n",
       "        0.00957741, -0.11276507, -0.02271997,  0.14445224, -0.00840628,\n",
       "       -0.03509223,  0.10303397, -0.00474818, -0.02849258, -0.00179971,\n",
       "        0.03541456, -0.03873183,  0.16289444,  0.09119155,  0.05133548,\n",
       "       -0.01153626,  0.10501225,  0.08953188, -0.18804191, -0.07482777,\n",
       "        0.0823573 , -0.21999559, -0.09153958,  0.10757563,  0.04247302,\n",
       "       -0.03051135, -0.05816598, -0.01022058,  0.06951102, -0.05417137,\n",
       "       -0.00557057,  0.08584974, -0.00350851, -0.01247351, -0.11682458,\n",
       "        0.18128015, -0.09628124, -0.06755148,  0.08739156, -0.12089624,\n",
       "       -0.10498443, -0.13962221, -0.04237365,  0.06679508, -0.13076897,\n",
       "       -0.05309304,  0.11943423, -0.02472617,  0.01019668,  0.21304236,\n",
       "       -0.08046678, -0.04623387,  0.20599511,  0.22544158,  0.13827723,\n",
       "        0.05658125,  0.09807704,  0.16142759,  0.05483067,  0.05857624,\n",
       "        0.1569227 , -0.02057766,  0.01919206,  0.05889341,  0.14323011,\n",
       "        0.00217596,  0.04449264,  0.19441608, -0.10302936,  0.05794261,\n",
       "       -0.05173578,  0.07966436,  0.03681327, -0.09077939,  0.05773795],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['academic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vp_academic', 0.9920320510864258),\n",
       " ('non_academic', 0.9616369605064392),\n",
       " ('climb_academic', 0.960375189781189),\n",
       " ('academic_cvs', 0.9587602615356445),\n",
       " ('climbs_academic', 0.9541186690330505),\n",
       " ('unacademic', 0.952389121055603),\n",
       " ('nonacademic', 0.9518829584121704),\n",
       " ('lambert_academic', 0.9488770961761475),\n",
       " ('bielefeld_academic', 0.9447492361068726),\n",
       " ('academic_theft', 0.9447307586669922)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar words to `academic` are now bigrams like `non academic` and unigrams like `nonacademic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
