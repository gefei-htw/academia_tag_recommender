{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook discusses multi label evaluation methods using the [academia.stackexchange.com](https://academia.stackexchange.com/) data dump.\n",
    "\n",
    "Evaluation metrics for multi label classification differ from binary classification metrics, since there are a few more questions to take into account. Therefore these metrics are divided into two main areas.\n",
    "\n",
    "## Table of Contents\n",
    "* [Example-based](#example_based)\n",
    "* [Label-based](#label_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_based'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example-based\n",
    "\n",
    "> The example-based evaluation measures are based on the average differences of the actual and the predicted sets of labels over all examples of the evaluation dataset. <cite>[(Madjaroj et al. 2012)][0]</cite>\n",
    "\n",
    "\n",
    "**Hamming loss**\n",
    "> The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
    "<cite>[scikit-learn][1]</cite>\n",
    "\n",
    "**Accuracy**\n",
    "> In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "<cite>[scikit-learn][2]</cite>\n",
    "\n",
    "**Precision**\n",
    "> The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "<cite>[scikit-learn][3]</cite>\n",
    "\n",
    "**Recall**\n",
    "> The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "<cite>[scikit-learn][4]</cite>\n",
    "\n",
    "**F1 score**\n",
    "> The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: *F1 = 2 * (precision * recall) / (precision + recall)*\n",
    "<cite>[scikit-learn][5]</cite>\n",
    "\n",
    "[0]: https://doi.org/10.1016/j.patcog.2012.03.004\n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html?highlight=hamming_loss\n",
    "[2]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy#sklearn.metrics.accuracy_score\n",
    "[3]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision#sklearn.metrics.precision_score\n",
    "[4]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score\n",
    "[5]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=label_based/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label-based\n",
    "\n",
    "> The label-based evaluation measures [...] assess the predictive performance for each label separately and then average the performance over all labels. \n",
    "<cite>[(Madjaroj et al. 2012)][0]</cite>\n",
    "\n",
    "**Micro**\n",
    "> Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "<cite>[scikit-learn][1]</cite>\n",
    "\n",
    "**Macro**\n",
    "> Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "<cite>[scikit-learn][1]</cite>\n",
    "\n",
    "[0]: https://doi.org/10.1016/j.patcog.2012.03.004\n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
