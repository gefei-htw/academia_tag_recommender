{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling & Multiple gridsearch\n",
    "\n",
    "This notebook discusses Multi-label classification methods using undersampling for the [academia.stackexchange.com](https://academia.stackexchange.com/).\n",
    "\n",
    "## Table of Contents\n",
    "* [Data import](#data_import)\n",
    "* [Data preparation](#data_preparation)\n",
    "* [Methods](#methods)\n",
    "* [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "from academia_tag_recommender.definitions import MODELS_PATH\n",
    "from academia_tag_recommender.classifier import Classifier, available_classifier_paths\n",
    "\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_import'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academia_tag_recommender.data import documents\n",
    "from academia_tag_recommender.test_train_data import get_X_reduced, get_y, get_test_train_data\n",
    "from academia_tag_recommender.preprocessing_definition import PreprocessingDefinition\n",
    "\n",
    "def get_X():\n",
    "    return np.vstack([document.text for document in documents])\n",
    "y = get_y()\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_test_train_data(get_X(), y, scale=False)\n",
    "print('Train set with shape ', X_train.shape)\n",
    "print('Test set with shape', X_test.shape)\n",
    "\n",
    "preprocessing = PreprocessingDefinition('tfidf', 'basic', 'basic', 'english', '1,1', 'TruncatedSVD')\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = get_test_train_data(get_X_reduced(preprocessing), y)\n",
    "\n",
    "preprocessing = PreprocessingDefinition('count', 'basic', 'basic', 'english', '1,1', 'TruncatedSVD')\n",
    "X_train_count, X_test_count, y_train_count, y_test_count = get_test_train_data(get_X_reduced(preprocessing), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preparation'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academia_tag_recommender.embedded_data import word2vec, doc2vec, fasttext2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(embedding):\n",
    "    if embedding == 'word2vec':\n",
    "        X_train_, X_test_ = word2vec(X_train, X_test)\n",
    "        return X_train_, X_test_, y_train, y_test\n",
    "    elif embedding == 'doc2vec':\n",
    "        X_train_, X_test_ = doc2vec(X_train, X_test, y)\n",
    "        return X_train_, X_test_, y_train, y_test\n",
    "    elif embedding == 'fasttext':\n",
    "        X_train_, X_test_ = fasttext2vec(X_train, X_test)\n",
    "        return X_train_, X_test_, y_train, y_test\n",
    "    elif embedding == 'tfidf':\n",
    "        return X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf\n",
    "    elif embedding == 'count':\n",
    "        return X_train_count, X_test_count, y_train_count, y_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = available_classifier_paths('classwise')\n",
    "def create_classifier(classifier_options, name, embedding):\n",
    "    path = [path for path in paths if name in path.name and embedding in path.name]\n",
    "    if len(path) > 0:\n",
    "        clf = load(path[0])\n",
    "    else:\n",
    "        preprocessing = PreprocessingDefinition(embedding, 'basic', 'basic', 'english', '1,1', 'None')\n",
    "        classifier = ClasswiseClassifier(classifier_options, embedding, undersample=True)\n",
    "        clf = Classifier(classifier, preprocessing, name)\n",
    "        X_train_, X_test_, y_train_, y_test_ = get_data(embedding)\n",
    "        clf.fit(X_train_, y_train_)\n",
    "        clf.score(X_test_, y_test_)\n",
    "        clf.save('classwise')\n",
    "    print('Training: {}s'.format(clf.training_time))\n",
    "    print('Test: {}s'.format(clf.test_time))\n",
    "    clf.evaluation.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This approach is similar to the approach shown in [notebook 4.5.3](4.5.3-me-classwise-undersampling.ipynb). But instead of training different classification methods individually, this [Classwise Classifier](../classwise_classifier.py) chooses the best performing classification method for each label. Therefore resulting in differint classification methods used in the final Classwise Classifier.\n",
    "\n",
    "For each label each of the three classification methods is trained and their specific tuning parameter found using [Gridsearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) as in [notebook 4.5.1](4.5.1-me-classwise-gridsearch.ipynb).\n",
    "- Linear SVC, optimizing parameter `C`\n",
    "- Logistic Regression, optimizing parameter `C`\n",
    "- MLPClassifier, optimizing parameter `alpha`\n",
    "\n",
    "[Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall#sklearn.metrics.recall_score) is used as  the scoring-function for the optimization. For reasons of that decision see [notebook 3.0](3.0-me-evaluation-metrics.ipynb).\n",
    "\n",
    "In addition undersampling is used for each individual label as in [notebook 4.5.2](4.5.2-me-classwise-undersampling.ipynb) Instead of handing all samples to the classifier, a more balanced ratio is used as training data. The [Classwise Classifier](../classwise_classifier.py) uses a ratio of 1:25, depending on how many positive samples are available in the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from academia_tag_recommender.classwise_classifier import ClasswiseClassifier, ClassifierOption\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_options = [ClassifierOption(LinearSVC(random_state=RANDOM_STATE), grid_search=True, parameter={'C':[0.1, 0.5, 1, 5, 10]}),\n",
    "                     ClassifierOption(LogisticRegression(random_state=RANDOM_STATE), grid_search=True, parameter={'C':[0.1, 0.5, 1, 5, 10]}),\n",
    "                     ClassifierOption(MLPClassifier(random_state=RANDOM_STATE), grid_search=True, paramater={'alpha':[0.0001, 0.001, 0.01, 0.1]})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classifier(classifier_options, 'ClasswiseClassifier(multiple)', 'word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classifier(classifier_options, 'ClasswiseClassifier(multiple)', 'doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classifier(classifier_options, 'ClasswiseClassifier(multiple)', 'fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classifier(classifier_options, 'ClasswiseClassifier(multiple)', 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classifier(classifier_options, 'ClasswiseClassifier(multiple)', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = available_classifier_paths('classwise')\n",
    "paths = [path for path in paths if 'multiple' in path.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = []\n",
    "for path in paths:\n",
    "    clf = load(path)\n",
    "    evaluation = clf.evaluation\n",
    "    evals.append([str(clf) + ' ' + str(clf.preprocessing.vectorizer), evaluation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = ['Precision', 'F1', 'Recall']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,10))\n",
    "axes[0].set_title('Sample')\n",
    "axes[1].set_title('Macro')\n",
    "axes[2].set_title('Micro')\n",
    "for eval_ in evals:\n",
    "    evaluator = eval_[1]\n",
    "    axes[0].plot(x_, [evaluator.precision_samples, evaluator.f1_samples, evaluator.recall_samples], label=eval_[0])\n",
    "    axes[1].plot(x_, [evaluator.precision_macro, evaluator.f1_macro, evaluator.recall_macro], label=eval_[0])\n",
    "    axes[2].plot(x_, [evaluator.precision_micro, evaluator.f1_micro, evaluator.recall_micro], label=eval_[0])\n",
    "axes[2].legend(bbox_to_anchor=(1, 1), ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3 = sorted(paths, key=lambda x: load(x).evaluation.recall_macro, reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_label_accuracy(orig, prediction):\n",
    "    if not isinstance(prediction, np.ndarray):\n",
    "        prediction = prediction.toarray()\n",
    "    l = 1 - orig - prediction\n",
    "    return np.average(l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classwise_results = []\n",
    "for clf_path in top_3:\n",
    "    clf = load(clf_path)\n",
    "    _, X_test_, _, y_test_ = get_data(clf.preprocessing.vectorizer)\n",
    "    prediction = clf.predict(X_test_)\n",
    "    label_accuracies = per_label_accuracy(y_test_, prediction)\n",
    "    report = classification_report(y_test_, prediction, output_dict=True, zero_division=0)\n",
    "    classwise_report = {}\n",
    "    for i, result in enumerate(report):\n",
    "        if i < len(label_accuracies):\n",
    "            classwise_report[result] = report[result]\n",
    "            classwise_report[result]['accuracy'] = label_accuracies[int(result)]\n",
    "    classwise_results.append((str(clf) + ' ' + str(clf.preprocessing.vectorizer), classwise_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_ = np.arange(0, len(y_test[0]))\n",
    "for classwise_result in classwise_results:\n",
    "    name, results = classwise_result\n",
    "    _, fig_1 = plt.subplots(figsize=(15,3))\n",
    "    fig_1.set_title(name)\n",
    "    sorted_results = sorted(results, key=lambda x: results[x]['support'], reverse=True)\n",
    "    fig_1.plot(x_, [results[result]['precision'] for result in sorted_results][0:len(x_)], label='Precision')\n",
    "    fig_1.plot(x_, [results[result]['recall'] for result in sorted_results][0:len(x_)], label='Recall')\n",
    "    fig_1.plot(x_, [results[result]['f1-score'] for result in sorted_results][0:len(x_)], label='F1')\n",
    "    fig_1.plot(x_, [results[result]['accuracy'] for result in sorted_results][0:len(x_)], label=\"Accuracy\")\n",
    "    fig_1.set_xlabel('label (sorted by support)')\n",
    "    fig_1.set_ylabel('score')\n",
    "    fig_1.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
