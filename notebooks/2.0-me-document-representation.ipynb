{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Representation\n",
    "\n",
    "This notebook evaluates methods for document representation using the [academia.stackexchange.com](https://academia.stackexchange.com/) data dump.\n",
    "\n",
    "The process of document representation is usually split into three steps: Preprocessing, Transformation and Dimension Reduction.\n",
    "\n",
    "## Table of Contents\n",
    "- [Preprocessing](#preprocessing)\n",
    "- [Transformation](#transformation)\n",
    "- [Dimension Reduction](#dim_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=preprocessing/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In text preprocessing the defines which characters of the documents text will be part of the document representation. That decision is split into the following parts:\n",
    "\n",
    "* **Removing characters**\n",
    "\n",
    "    In a preprocessing step parts of the text can be removed that should not be part of the document representation (e.g. html tags, numbers, punctuation).\n",
    "\n",
    "* **Tokenization**:\n",
    "\n",
    "    In the tokenization step the text gets split into a vector of tokens. It needs to be decided where to split the text (e.g. spaces, punctuation) and what text should appear in the vector (e.g. minimum length). Furthermore it can be decided if unigrams (one word), bigrams (phrases consisting of two words) or even trigrams (phrases consisting of three words) should be tokens (called 'ngrams').\n",
    "\n",
    "* **Normalization**\n",
    "    \n",
    "    Another decision has to made regarding how to differentiate between tokens. It might be not relevant whether text includes *student* or *students*, so it might be a could option to choose the same representation for both of them. This step is called *Normalization*. There are different normalization algorithms that either stem words to their word stem (e.g. Porter Stemmer, Lancaster Stemmer) or choose a generell represation for words describing the same topic (Lemmatizer).\n",
    "\n",
    "* **Stop word removal**\n",
    "\n",
    "    In natural speech there are words that appear often regardless of the meaning of the text. For text classification those might not be relevant (e.g. articles, pronouns) and thuse be removed from the document representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=transformation/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "\n",
    "The process of text transformation (also called feature extraction) converts the tokens of the text into a vector representation to feed to a classifier. There are two often used approaches to text transformation:\n",
    "\n",
    "- [Bag of words](2.1-me-bag-of-words.ipynb)\n",
    "- [Embedding](2.2-me-embedding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dim_reduce/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "\n",
    "Especially the Bag of words approach can result in a vector of very high dimensionality. If each word in a corpus is used as a feature, the number of features used would be as high as the number of words in the corpus. Therefore is might be necessary to [reduce the dimensions](2.3-me-dimensionality-reduction.ipynb) (also called feature selection)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
