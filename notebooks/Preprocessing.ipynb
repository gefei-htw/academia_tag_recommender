{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## This Notebook handles data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from joblib import dump, load\n",
    "\n",
    "from academia_tag_recommender.documents import documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from academia_tag_recommender.preprocessor import BasicPreprocessor\n",
    "from academia_tag_recommender.tokenizer import BasicTokenizer, EnglishStemmer, PorterStemmer, LancasterStemmer, Lemmatizer\n",
    "from academia_tag_recommender.vectorizer_computation import get_vect_feat_with_params\n",
    "\n",
    "texts = list(map(lambda x: x.text, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "* Vectorizer `CountVectorizer`, `TfidfVectorizer`\n",
    "* Preprocessor `None`, `BasicPreprocessor` \n",
    "* Tokenizer\n",
    "    * without normalization: `None`, `BasicTokenizer`\n",
    "    * with normalization:\n",
    "        * Stemmer: `EnglishStemmer`, `PorterStemmer`, `LancasterStemmer`\n",
    "        * Lemmatizer: `Lemmatizer`\n",
    "* Stopwords `None`, `'english'`\n",
    "* NGrams `(1, 1)`, `(1, 2)`, `(1, 3)`, `(2, 2)`, `(2, 3)`, `(3, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "preprocessors = [None, BasicPreprocessor]\n",
    "tokenizers = [None, BasicTokenizer, EnglishStemmer, PorterStemmer, Lemmatizer]\n",
    "stop_word_options = [None, 'yes']\n",
    "n_gram_options = [(1,1), (1,2), (2,2), (1,3), (2,3), (3,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document representation models can be accessed using\n",
    "[vectorizer, features] = get_vect_feat_with_params(texts, CountVectorizer, BasicTokenizer, None, 'yes', (1, 1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load('../models/document_representation/results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [result for result in results if result['n_grams'] == '(1, 1)']\n",
    "bigrams = [result for result in results if result['n_grams'] == '(2, 2)']\n",
    "trigrams = [result for result in results if result['n_grams'] == '(3, 3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBarchart(feature):\n",
    "    labels = ['none', 'BasicTokenizer', 'EnglishStemmer', 'PorterStemmer', 'Lemmatizer']\n",
    "    stopwords_with_p = []\n",
    "    no_stopwords_with_p = []\n",
    "    stopwords_without_p = []\n",
    "    no_stopwords_without_p = []\n",
    "    for label in labels:\n",
    "        for unigram in unigrams:\n",
    "            if unigram['v'] == 'CountVectorizer':\n",
    "                if unigram['t'] == label:\n",
    "                    value = unigram[feature]\n",
    "                    if feature == 'shape':\n",
    "                        value = value[1]                        \n",
    "                    if unigram['stopwords'] == 'yes':\n",
    "                        if unigram['p'] == 'BasicPreprocessor':\n",
    "                            stopwords_with_p.append(value)\n",
    "                        else:\n",
    "                            stopwords_without_p.append(value)\n",
    "                    else:\n",
    "                        if unigram['p'] == 'BasicPreprocessor':\n",
    "                            no_stopwords_with_p.append(value)\n",
    "                        else:\n",
    "                            no_stopwords_without_p.append(value)\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.20\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.bar(x - width*2, no_stopwords_without_p, width, label='p = none, stopwords = none')\n",
    "    ax.bar(x - width, stopwords_without_p, width, label='p = none, stopwords = yes')\n",
    "    ax.bar(x, no_stopwords_with_p, width, label='p = BasicPreprocessor, stopwords = none')\n",
    "    ax.bar(x + width, stopwords_with_p, width, label='p = BasicPreprocessor, stopwords = yes')\n",
    "\n",
    "    ax.set_ylabel('{}'.format(feature.capitalize()))\n",
    "    ax.set_title('{} by Tokenizer w/o Preprocessor and w/o Stopwords'.format(feature.capitalize()))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarchart('shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBarchart('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer\n",
    "p = BasicPreprocessor\n",
    "ts = [None, BasicTokenizer, EnglishStemmer, PorterStemmer, Lemmatizer]\n",
    "stopwords = 'yes'\n",
    "ngrams = '(1, 1)'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for tokenizer in ts:\n",
    "    [vectorizer, features] = get_vect_feat_with_params(texts, v, tokenizer, p, stopwords, ngrams, False)\n",
    "\n",
    "    sum_words = features.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = list(sorted(words_freq, key = lambda x: x[1], reverse=True))\n",
    "    words_freq = [(word, freq) for (word, freq) in words_freq if freq > 200]\n",
    "    stepSize = 1\n",
    "    x = np.arange(len(words_freq))\n",
    "    y = [freq for (word, freq) in words_freq]\n",
    "\n",
    "    ax.plot(x,y)\n",
    "\n",
    "plt.ylabel('count')\n",
    "plt.legend([t.__name__ if t else 'None' for t in ts])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(1, 1), (2, 10), (11, 100), (101, 1000), (1001, 10000), (10001, 100000), (1001, 2000), (2001, 3000), (3001, 4000), (4001, 5000), (5001, 6000), (6001, 7000), (7001, 8000), (8001, 9000), (9001, 10000), (10001, 100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequency(data, start, end):\n",
    "    freq = len([(word, freq) for (word, freq) in data if freq >= start and freq <= end])\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencies(data):\n",
    "    stats = {}\n",
    "    stats['words'] = len(data)\n",
    "    stats['min'] = data[len(data)-1][1]\n",
    "    stats['max'] = data[0][1]\n",
    "    for range_ in ranges:\n",
    "        [start, end] = range_\n",
    "        stats['[{}-{}]'.format(start, end)] = getFrequency(data, start, end)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(v, p, t, stopwords, ngrams): \n",
    "    #print('v={} p={} t={} stopwords={} ngrams={}'.format(v, p, t, stopwords, ngrams))\n",
    "    features = load('models/features/v={}&p={}&t={}&stopwords={}&ngrams={}.joblib'.format(v, p, t, stopwords, ngrams))\n",
    "    vectorizer = load('models/vectorizer/v={}&p={}&t={}&stopwords={}&ngrams={}.joblib'.format(v, p, t, stopwords, ngrams))\n",
    "\n",
    "    #print('removed words: {}'.format(len(vectorizer.stop_words_)))\n",
    "    #print('shape: {}'.format(features.shape))\n",
    "    sum_words = features.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = list(sorted(words_freq, key = lambda x: x[1], reverse=True))\n",
    "    #print(words_freq[:10])\n",
    "    #print(words_freq[len(words_freq)-10:])\n",
    "    return getFrequencies(words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats['none'] = analyze('CountVectorizer', 'BasicPreprocessor', 'none', 'yes', '(1, 1)')\n",
    "stats['BasicTokenizer'] = analyze('CountVectorizer', 'BasicPreprocessor', 'BasicTokenizer', 'yes', '(1, 1)')\n",
    "stats['EnglishStemmer'] = analyze('CountVectorizer', 'BasicPreprocessor', 'EnglishStemmer', 'yes', '(1, 1)')\n",
    "stats['PorterStemmer'] = analyze('CountVectorizer', 'BasicPreprocessor', 'PorterStemmer', 'yes', '(1, 1)')\n",
    "stats['Lemmatizer'] = analyze('CountVectorizer', 'BasicPreprocessor', 'Lemmatizer', 'yes', '(1, 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format('Range', 'none', 'BasicTokenizer', 'EnglishStemmer', 'PorterStemmer', 'Lemmatizer'))\n",
    "print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format('Words', stats['none']['words'], stats['BasicTokenizer']['words'], stats['EnglishStemmer']['words'], stats['PorterStemmer']['words'], stats['Lemmatizer']['words']))\n",
    "for range_ in ranges:\n",
    "    [start, end] = range_\n",
    "    key = '[{}-{}]'.format(start, end)\n",
    "    print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(key, stats['none'][key], stats['BasicTokenizer'][key], stats['EnglishStemmer'][key], stats['PorterStemmer'][key], stats['Lemmatizer'][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats['none'] = analyze('TfidfVectorizer', 'BasicPreprocessor', 'none', 'yes', '(1, 1)')\n",
    "stats['BasicTokenizer'] = analyze('TfidfVectorizer', 'BasicPreprocessor', 'BasicTokenizer', 'yes', '(1, 1)')\n",
    "stats['EnglishStemmer'] = analyze('TfidfVectorizer', 'BasicPreprocessor', 'EnglishStemmer', 'yes', '(1, 1)')\n",
    "stats['PorterStemmer'] = analyze('TfidfVectorizer', 'BasicPreprocessor', 'PorterStemmer', 'yes', '(1, 1)')\n",
    "stats['Lemmatizer'] = analyze('TfidfVectorizer', 'BasicPreprocessor', 'Lemmatizer', 'yes', '(1, 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format('Range', 'none', 'BasicTokenizer', 'EnglishStemmer', 'PorterStemmer', 'Lemmatizer'))\n",
    "print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format('Words', stats['none']['words'], stats['BasicTokenizer']['words'], stats['EnglishStemmer']['words'], stats['PorterStemmer']['words'], stats['Lemmatizer']['words']))\n",
    "for range_ in ranges:\n",
    "    [start, end] = range_\n",
    "    key = '[{}-{}]'.format(start, end)\n",
    "    print('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(key, stats['none'][key], stats['BasicTokenizer'][key], stats['EnglishStemmer'][key], stats['PorterStemmer'][key], stats['Lemmatizer'][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
